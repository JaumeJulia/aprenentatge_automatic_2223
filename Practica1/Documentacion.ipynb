{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Práctica 1 - Clasificación de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Misc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean\n",
    "from matplotlib.colors import ListedColormap\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "#CrossValidation\n",
    "##K-folding\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "##GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "#https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# Model\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos\n",
    "En esta primera parte de la práctica vamos a preparar los datos para poder hacer la clasificación. Para ello haremos que cada muestra siga el siguiente formato: $C_1$ | $C_2$ | $C_3$ | .. | $C_n$ | $y$  (siendo $C$ una característica y $y$ el target).\n",
    "\n",
    "Se va a codificar $y$ con los valores \"0\" para representar la clase _catala_ y \"1\" para la clase _angles_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Auxiliar function that reformats df to one that is more fitting to Classification Problems\n",
    "def reformat(dataFrame):\n",
    "    if dataFrame.columns[0]==\"catala\":\n",
    "        dataFrame['y']=0\n",
    "    else:\n",
    "        dataFrame['y']=1\n",
    "    \n",
    "    dataFrame.rename(columns={dataFrame.columns[0]: 'word'}, inplace=True)\n",
    "    return dataFrame\n",
    "\n",
    "#RawData, it has been a little bit formatted before reading the csv, to facilitate the process.\n",
    "raw=pd.read_csv(\"data/data.csv\")\n",
    "#Split df\n",
    "catala, angles= raw.filter(['catala'], axis=1), raw.filter(['angles'], axis=1)\n",
    "#Reformating\n",
    "catala=reformat(catala)\n",
    "angles=reformat(angles)\n",
    "\n",
    "#Merging\n",
    "wordsDF=pd.concat([catala,angles], axis=0)\n",
    "#Shuffle the rows\n",
    "wordsDF = wordsDF.sample(frac=1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracterísiticas\n",
    "Las características que hemos pensado que pueden ser útiles para la clasificación de las palabras son:\n",
    "- Cantidad de caracteres (Númerica)\n",
    "- Proporción de consonantes por vocal (consonantes / vocales) (Numérica)\n",
    "- Contiene patrones o normas ortográficas de una lengua de las que vamos a clasificar?\n",
    "    + Doble uso de vocal consecutivamente como es el caso del inglés (Categórica)\n",
    "    + Acentos en caso de catalán (Categórica)\n",
    "    + Contiene combinaciones de consonantes (consonant clusters) propias del inglés? (Categórica)\n",
    "\n",
    "Referencias\n",
    "- <Consonant_Clusters :https://www.aprendeinglessila.com/2013/09/consonantes-ingles-clusters/>\n",
    "- <Frecuencia_De_Letras_Usadas_En_Catalan: https://es.sttmedia.com/frecuencias-de-letras-catalan>\n",
    "- <Frecuencia_De_Letras_Usadas_En_Catalan: https://www3.nd.edu/~busiforc/handouts/cryptography/letterfrequencies.html>\n",
    "\n",
    "Para añadir las columnas que representen estas características, hemos aplicado las siguientes funciones a las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.to_string of           word   ratio  cantidadLetras  gotAccent  doubleVocal  enCC  y\n",
      "0       trobar  0.3333               6          0            0     0  0\n",
      "1       direct  0.3333               6          0            0     0  1\n",
      "2       parlar  0.3333               6          0            0     0  0\n",
      "3        class  0.2000               5          0            0     0  1\n",
      "4     position  0.5000               8          0            0     0  1\n",
      "...        ...     ...             ...        ...          ...   ... ..\n",
      "1971        si  0.5000               2          0            0     0  0\n",
      "1972     begin  0.4000               5          0            0     0  1\n",
      "1973      wife  0.5000               4          0            0     0  1\n",
      "1974  caràcter  0.2500               8          1            0     0  0\n",
      "1975     causa  0.6000               5          0            0     0  0\n",
      "\n",
      "[1976 rows x 7 columns]>\n"
     ]
    }
   ],
   "source": [
    "#ratio de consonantes y vocales\n",
    "def ratio (word):\n",
    "    vocals=0\n",
    "    for c in word:\n",
    "        if isVocal(c):\n",
    "            vocals+=1\n",
    "    return round(vocals/len(word), 4)\n",
    "\n",
    "#isVocal?\n",
    "def isVocal(c):\n",
    "    if(c=='a' or c=='e' or c=='i' or c=='o' or c=='u'):\n",
    "        return True\n",
    "    return \n",
    "#gotAccent?\n",
    "def gotAccent(word):\n",
    "    #List containing all possible accentuated chars from Catalan\n",
    "    accentuatedChars=[ord('à'),ord('è'),ord('é'),ord('í'),ord('ò'),ord('ó'),ord('ú')]\n",
    "    for c in word:\n",
    "        if ord(c) in accentuatedChars:\n",
    "            return 1\n",
    "    return 0\n",
    "def doubleVocal(word):\n",
    "    ocurrences=[\"aa\",\"ee\",\"ii\",\"oo\",\"uu\"]\n",
    "    for oc in ocurrences:\n",
    "        if word.find(oc)!=-1:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def enCC(word):\n",
    "    ocurrences=[\"sch\",\"spl\",\"shr\",\"squ\",\"thr\",\"spr\",\"scr\",\"sph\",\"th\",\"tw\",\"sw\",\"sk\",\"sm\"]\n",
    "    for oc in ocurrences:\n",
    "        if word.find(oc)!=-1:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "#Features Adding\n",
    "wordsDF['ratio']=wordsDF['word'].apply(ratio)\n",
    "wordsDF['cantidadLetras']=wordsDF['word'].apply(len)\n",
    "wordsDF['gotAccent']=wordsDF['word'].apply(gotAccent)\n",
    "wordsDF['doubleVocal']=wordsDF['word'].apply(doubleVocal)\n",
    "wordsDF['enCC']=wordsDF['word'].apply(enCC)\n",
    "#Reorganize DF\n",
    "wordsDF=wordsDF[['word','ratio','cantidadLetras','gotAccent','doubleVocal','enCC','y']]\n",
    "#Write DF to csv\n",
    "wordsDF.to_csv('data/definitiveData.csv', index=False)\n",
    "#Checking\n",
    "print(wordsDF.to_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación del conjunto de datos en entrenamiento y test\n",
    "Vamos a escoger dos tercios para el conjunto de entrenamiento y el resto para el test.\n",
    " Puntualizar que se han normalizado los datos para un mejor rendimiento del modelo. Solo se han escalado los datos numéricos, ya que sería erróneo escalar todo el dataset, con los categóricos, ya que podría dar lugar a dar menos importancia a estos últimos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we must separate dataframe into X and y format\n",
    "X=wordsDF.iloc[:,1:6]\n",
    "X_aux=wordsDF.iloc[:,1:3]\n",
    "y=wordsDF.iloc[:,6]\n",
    "\n",
    "#For better perfomance, we scale the data using an standard scaler\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(X_aux)\n",
    "standardData_aux = scaler.transform(X_aux)\n",
    "\n",
    "#Standarized DF\n",
    "standardData=X\n",
    "\n",
    "#Substitute standarized values in correspondent columns\n",
    "standardData['ratio']=standardData_aux[:,0]\n",
    "standardData['cantidadLetras']=standardData_aux[:,1]\n",
    "\n",
    "#Then we separate the data frame in training and test (will be used in chosen model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardData, y, test_size=0.33, random_state=33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "En este problema creemos que el mejor modelo para clasificar las palabras va a ser una SVC con un kernel gaussiano (RBG) ya que tenemos un número de características bastante bajo en comparación al número de muestras. (Podríamos poner una prueba en la que probamos el kernel líneal y comprobar que nos da peores valores).\n",
    "\n",
    "Los híperparámetros que deberemos de configurar en el modelo del SVM son:\n",
    "- _C_\n",
    "- _max\\_iter_\n",
    "- _gamma_\n",
    "\n",
    "El resto de híperparámetros no son tan necesarios ya que no se ajustan al problema.(Explicar algún que otro parámetro y decir por qué no es útil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "SVM_rbf = SVC(kernel='rbf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation\n",
    "Para el Nested Cross-Validation haremos lo siguiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score of Nested cross-validation:0.636+-0.029\n",
      "The best estimator is:SVC(C=1, gamma=1, max_iter=1000)\n"
     ]
    }
   ],
   "source": [
    "#Hyper Grid\n",
    "hyper_grid = {\"C\":[0.1,1,10],\"max_iter\":[100,1000,10000],\"gamma\":[.01,.1,1]}\n",
    "#Inner and Outer crossvalidation, following steps from :\n",
    "#https://www.analyticsvidhya.com/blog/2021/03/a-step-by-step-guide-to-nested-cross-validation/\n",
    "# and\n",
    "# https://inria.github.io/scikit-learn-mooc/python_scripts/cross_validation_nested.html\n",
    "\n",
    "inner_cv=StratifiedKFold(n_splits=3, shuffle=False)\n",
    "outer_cv=StratifiedKFold(n_splits=5, shuffle=False)\n",
    "#GridSearch definition\n",
    "search=GridSearchCV(estimator=SVM_rbf,cv=inner_cv,param_grid=hyper_grid,n_jobs=-1)\n",
    "search.fit(X_train,y_train)\n",
    "#Test score after nesting\n",
    "test_score=cross_val_score(search,X_train,y_train,cv=outer_cv,n_jobs=-1)\n",
    "\n",
    "#Printing results of Nested Cross-Validation\n",
    "print(f\"Mean score of Nested cross-validation:\"f\"{test_score.mean():.3f}+-{test_score.std():.3f}\")\n",
    "print(f\"The best estimator is:\"f\"{search.best_estimator_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del Modelo con los parámetros encontrados\n",
    "\n",
    "Una vez hemos encontrado los mejores híperparámetros para el SVM con Kernel Gaussiano, procedemos a entrenarlo y a realizar la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rati d'acerts en el bloc de predicció: 0.6584992343032159\n"
     ]
    }
   ],
   "source": [
    "#Definition of SVM with best hyperparamters\n",
    "SVM_rbf=SVC(kernel=\"rbf\",C=10,gamma=0.01,max_iter=1000)\n",
    "#Train Model with Train set\n",
    "SVM_rbf.fit(X_train,y_train)\n",
    "#Predict with X_test\n",
    "y_predicted=SVM_rbf.predict(X_test)\n",
    "differences=y_predicted-y_test\n",
    "errors = np.count_nonzero(differences)\n",
    "#Print results\n",
    "print(f'Rati d\\'acerts en el bloc de predicció: {(len(y_predicted)-errors)/len(y_predicted)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('aprenentatge_automatic_2223')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3ed45ef713d66eb30c4c224ecca975f6d1f36637243a303e315f56b339dc2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
