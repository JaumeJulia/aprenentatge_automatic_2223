{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Práctica 1 - Clasificación de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "import sklearn.preprocessing\n",
    "from numpy import std\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "#https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos\n",
    "En esta primera parte de la práctica vamos a preparar los datos para poder hacer la clasificación. Para ello haremos que cada muestra siga el siguiente formato: C1 | C2 | C3 | .. | CN | Y  (siendo C una característica y Y el target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Auxiliar function that reformats df to one that is more fitting to Classification Problems\n",
    "def reformat(dataFrame):\n",
    "    dataFrame['y']=dataFrame.columns[0]\n",
    "    dataFrame.rename(columns={dataFrame.columns[0]: 'word'}, inplace=True)\n",
    "    return dataFrame\n",
    "\n",
    "#RawData, it has been a little bit formatted before reading the csv, to facilitate the process.\n",
    "raw=pd.read_csv(\"data/data.csv\")\n",
    "#Split df\n",
    "catala, angles= raw.filter(['catala'], axis=1), raw.filter(['angles'], axis=1)\n",
    "#Reformating\n",
    "catala=reformat(catala)\n",
    "angles=reformat(angles)\n",
    "\n",
    "#Merging\n",
    "wordsDF=pd.concat([catala,angles], axis=0)\n",
    "#Shuffle the rows\n",
    "wordsDF = wordsDF.sample(frac=1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracterísiticas\n",
    "Las características que hemos pensado que pueden ser útiles para la clasificación de las palabras son:\n",
    "- Cantidad de caracteres (Númerica)\n",
    "- Proporción de consonantes por vocal (consonantes / vocales) (Numérica)\n",
    "- Contiene patrones o normas ortográficas de una lengua de las que vamos a clasificar?\n",
    "    + Doble uso de vocal consecutivamente como es el caso del inglés (Categórica)\n",
    "    + Acentos en caso de catalán (Categórica)\n",
    "    + Contiene combinaciones de consonantes (consonant clusters) propias del inglés? (Categórica)\n",
    "\n",
    "Referencias\n",
    "- <Consonant_Clusters :https://www.aprendeinglessila.com/2013/09/consonantes-ingles-clusters/>\n",
    "- <Frecuencia_De_Letras_Usadas_En_Catalan: https://es.sttmedia.com/frecuencias-de-letras-catalan>\n",
    "- <Frecuencia_De_Letras_Usadas_En_Catalan: https://www3.nd.edu/~busiforc/handouts/cryptography/letterfrequencies.html>\n",
    "\n",
    "Para añadir las columnas que representen estas características, hemos aplicado las siguientes funciones a las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.to_string of           word   ratio  cantidadLetras  gotAccent  doubleVocal  enCC       y\n",
      "0         verb  0.2500               4          0            0     0  catala\n",
      "1      arribar  0.4286               7          0            0     0  catala\n",
      "2        capaç  0.4000               5          0            0     0  catala\n",
      "3           em  0.5000               2          0            0     0  catala\n",
      "4       màster  0.1667               6          1            0     0  catala\n",
      "...        ...     ...             ...        ...          ...   ...     ...\n",
      "1971  filferro  0.3750               8          0            0     0  catala\n",
      "1972      each  0.5000               4          0            0     0  angles\n",
      "1973     lluna  0.4000               5          0            0     0  catala\n",
      "1974      love  0.5000               4          0            0     0  angles\n",
      "1975    trobar  0.3333               6          0            0     0  catala\n",
      "\n",
      "[1976 rows x 7 columns]>\n"
     ]
    }
   ],
   "source": [
    "#ratio de consonantes y vocales\n",
    "def ratio (word):\n",
    "    vocals=0\n",
    "    for c in word:\n",
    "        if isVocal(c):\n",
    "            vocals+=1\n",
    "    return round(vocals/len(word), 4)\n",
    "\n",
    "#isVocal?\n",
    "def isVocal(c):\n",
    "    if(c=='a' or c=='e' or c=='i' or c=='o' or c=='u'):\n",
    "        return True\n",
    "    return \n",
    "#gotAccent?\n",
    "def gotAccent(word):\n",
    "    #List containing all possible accentuated chars from Catalan\n",
    "    accentuatedChars=[ord('à'),ord('è'),ord('é'),ord('í'),ord('ò'),ord('ó'),ord('ú')]\n",
    "    for c in word:\n",
    "        if ord(c) in accentuatedChars:\n",
    "            return 1\n",
    "    return 0\n",
    "def doubleVocal(word):\n",
    "    ocurrences=[\"aa\",\"ee\",\"ii\",\"oo\",\"uu\"]\n",
    "    for oc in ocurrences:\n",
    "        if word.find(oc)!=-1:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def enCC(word):\n",
    "    ocurrences=[\"sch\",\"spl\",\"shr\",\"squ\",\"thr\",\"spr\",\"scr\",\"sph\",\"th\",\"tw\",\"sw\",\"sk\",\"sm\"]\n",
    "    for oc in ocurrences:\n",
    "        if word.find(oc)!=-1:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "#Features Adding\n",
    "wordsDF['ratio']=wordsDF['word'].apply(ratio)\n",
    "wordsDF['cantidadLetras']=wordsDF['word'].apply(len)\n",
    "wordsDF['gotAccent']=wordsDF['word'].apply(gotAccent)\n",
    "wordsDF['doubleVocal']=wordsDF['word'].apply(doubleVocal)\n",
    "wordsDF['enCC']=wordsDF['word'].apply(enCC)\n",
    "#Reorganize DF\n",
    "wordsDF=wordsDF[['word','ratio','cantidadLetras','gotAccent','doubleVocal','enCC','y']]\n",
    "wordsDF.to_csv('data/definitiveData.csv', index=False)\n",
    "print(wordsDF.to_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación del conjunto de datos en entrenamiento y test\n",
    "Vamos a escoger dos tercios para el conjunto de entrenamiento y el resto para el test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.07855609 -0.66949106]\n",
      " [ 0.37096326  0.94974313]\n",
      " [ 0.13884538 -0.12974633]\n",
      " ...\n",
      " [ 0.13884538 -0.12974633]\n",
      " [ 0.95044636 -0.66949106]\n",
      " [-0.40249248  0.4099984 ]]\n",
      "[-1.07855609  0.37096326  0.13884538 ...  0.13884538  0.95044636\n",
      " -0.40249248]\n",
      "[-0.66949106  0.94974313 -0.12974633 ... -0.12974633 -0.66949106\n",
      "  0.4099984 ]\n",
      "         ratio  cantidadLetras  gotAccent  doubleVocal  enCC\n",
      "0    -1.078556       -0.669491          0            0     0\n",
      "1     0.370963        0.949743          0            0     0\n",
      "2     0.138845       -0.129746          0            0     0\n",
      "3     0.950446       -1.748981          0            0     0\n",
      "4    -1.754620        0.409998          1            0     0\n",
      "...        ...             ...        ...          ...   ...\n",
      "1971 -0.064055        1.489488          0            0     0\n",
      "1972  0.950446       -0.669491          0            0     0\n",
      "1973  0.138845       -0.129746          0            0     0\n",
      "1974  0.950446       -0.669491          0            0     0\n",
      "1975 -0.402492        0.409998          0            0     0\n",
      "\n",
      "[1976 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#First we must separate dataframe into X and y format\n",
    "X=wordsDF.iloc[:,1:6]\n",
    "X_aux=wordsDF.iloc[:,1:3]\n",
    "y=wordsDF.iloc[:,6]\n",
    "#For better perfomance, we scale the data using an standard scaler\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(X_aux)\n",
    "standardData_aux = scaler.transform(X_aux)\n",
    "print(standardData_aux)\n",
    "#Ratio Standarized\n",
    "print(standardData_aux[:,0])\n",
    "#NumLletres Standarized\n",
    "print(standardData_aux[:,1])\n",
    "standardData=X\n",
    "standardData['ratio']=standardData_aux[:,0]\n",
    "standardData['cantidadLetras']=standardData_aux[:,1]\n",
    "print(standardData)\n",
    "#Then we separate the data frame in training and test (will be used in chosen model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardData, y, test_size=0.33, random_state=33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "En este problema creemos que el mejor modelo para clasificar las palabras va a ser una SVC con un kernel gaussiano (RBG) ya que tenemos un número de características bastante bajo en comparación al número de muestras. \n",
    "\n",
    "Los híperparámetros que deberemos de configurar en el modelo del SVM Lineal son:\n",
    "- _penalty_\n",
    "- _loss_\n",
    "- _C_\n",
    "- _max_iter_\n",
    "\n",
    "El resto de híperparámetros no son tan necesarios ya que no se ajustan al problema. Por ejemplo _dual_  no nos va a dar ninguna diferencia, ya que es un booleano que indica que algoritmo usar. Se usa **dual = False** cuando el número de muestras es mayor al de características, que es nuestro caso. <br>Tampoco usaremos *multi_class* ya que en este experimento solo usaremos las clases \"catalan\" y \"angles\".\n",
    "\n",
    "Para el ajuste de estos híperparámetros haremos un _nested cross-validation_ que consiste en combinar **GridSearch** junto a **K-Folding**, pero en este caso usaremos la variación _Stratified_ para mantener una distribución balanceada de las clases en cada fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo\n",
    "SVM_rbf = SVC(C=50,kernel='rbf',max_iter=1000, gamma=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Folding\n",
    "Lo primero que debemos de hacer es decidir qué valor de _k_ vamos a utilizar. Normalmente se utilizan valores entre 5 y 10. Vamos a probar entre estos rangos y ver qué valor nos da mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> folds=5, accuracy = 0.6462, (0.6025, 0.6793)\n",
      "-> folds=6, accuracy = 0.6386, (0.5957, 0.6818)\n",
      "-> folds=7, accuracy = 0.6452, (0.6064, 0.689)\n",
      "-> folds=8, accuracy = 0.6392, (0.587, 0.6802)\n",
      "-> folds=9, accuracy = 0.6427, (0.589, 0.6849)\n",
      "-> folds=10, accuracy = 0.6301, (0.5635, 0.7056)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(cv):\n",
    "    #https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "    score = cross_val_score(SVM_rbf,X,y,scoring='accuracy',cv=cv,n_jobs=-1)\n",
    "    return mean(score),score.min(),score.max()\n",
    "\n",
    "\n",
    "#Range to be tested\n",
    "folds = range (5,11)\n",
    "for k in folds:\n",
    "    #Shuffle not needed because dataset is not sorted by class \n",
    "    cv=StratifiedKFold(n_splits=k, shuffle=False, random_state=1)\n",
    "    k_mean, k_min, k_max = evaluate_model(cv)\n",
    "    print(f'-> folds={k}, accuracy = {round(k_mean,4)}, ({round(k_min,4)}, {round(k_max,4)})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GridSearchLayout"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#GridSearchCV(SVM_rbf, [1,0], cv=None, verbose=0)\n",
    "\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "#cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(standardData):\n",
    " # split data\n",
    " X_train, X_test = standardData[train_ix, :], standardData[test_ix, :]\n",
    " y_train, y_test = y[train_ix], y[test_ix]\n",
    " # configure the cross-validation procedure\n",
    " cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    " # define the model\n",
    " SVM_rbf = SVC(C=50,kernel='rbf',max_iter=1000, gamma=1)\n",
    " # define search space\n",
    " param_grid = dict()\n",
    " param_grid['n_estimators'] = [10, 100, 500]\n",
    " param_grid['max_features'] = [2, 4, 6]\n",
    " # define search\n",
    " search = GridSearchCV(SVM_rbf, param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    " #search = GridSearchCV(SVM_rbf, param_grid, cv=None, verbose=0)\n",
    " # execute search\n",
    " result = search.fit(X_train, y_train)\n",
    " # get the best performing model fit on the whole training set\n",
    " best_model = result.best_estimator_\n",
    " # evaluate model on the hold out dataset\n",
    " yhat = best_model.predict(X_test)\n",
    " # evaluate the model\n",
    " acc = accuracy_score(y_test, yhat)\n",
    " # store the result\n",
    " outer_results.append(acc)\n",
    " # report progress\n",
    " print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('aprenentatge_automatic_2223')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3ed45ef713d66eb30c4c224ecca975f6d1f36637243a303e315f56b339dc2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
